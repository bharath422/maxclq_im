\section{Clique Algorithms and Community Detection}
\label{sec:applications}

%In this section we demonstrate how clique finding algorithms can be used as parts of methods for detecting overlapping communities in networks. 
In this section we demonstrate how clique finding algorithms can be used for detecting overlapping communities in networks. 


{\bf Background. }
Most community detection algorithms are designed to identify mutually independent communities in a given network and therefore are not suitable for detecting overlapping communities. Yet, in many real-world networks, it is natural to find vertices (or members) that belong to more than one group (or community) at the same time.

Palla et al~\cite{cite-key} introduced the Clique Percolation Method (CPM) 
as one effective approach for detecting overlapping communities in a network. 
The basic premise in CPM is that a typical community is likely to be made up of several cliques that share many of their vertices.  
We recall a few notions defined in \cite{cite-key} to make this more precise.
A clique of size $k$ is called a {\em $k$-clique}, and   
two $k$-cliques are called {\em adjacent} if they share $k - 1$ nodes. 
A {\em $k$-clique community} is a union of all $k$-cliques that can be reached 
from each other through a series of adjacent $k$-cliques. 
Further, a {\em $k$-clique chain} is the union of a sequence of adjacent $k$-cliques, and
two $k$-cliques are said to be $k$-clique-connected if they are parts of a $k$-clique chain. With these notions at hand, Palla et al devise a method to extract the $k$-clique communities, which  are essentially the $k$-clique connected components of the network; 
note that, by definition, $k$-clique communities allow for
overlaps between communities.

{\bf The CPM algorithm is illustrated in Figure \ref{fig-cpm}. Given a graph, it first extracts cliques of size $k$, in this case we choose $k$=3. This is followed by generating the {\it clique graph}, where each $k$-clique in the original graph is represented by a vertex. An edge is added between any two $k$-cliques in the clique graph that are adjacent, in the case of $k$=3, this means any two cliques that share two nodes. The connected components in the clique graph represent a community, and the actual members of the community are obtained by merging the vertices of the individual cliques that form the connected component. }

%Palla et al observe that the $k$-clique communities are equivalent to the $k$-clique connected components of the network; note that, by definition, $k$-clique communities allow for overlaps between communities to exist.


\begin{figure}%[h!]
  \centering
    %\includegraphics[width=1\textwidth]{communities_fb.pdf}
    \includegraphics[scale=0.4]{cpm.eps}
    
%\vspace{-30pt}
  \caption{Illustration of the Clique Percolation Method (CPM) algorithm on a sample graph.}
\label{fig-cpm}
\end{figure}
%\vspace{-10pt}


A large clique of size $q \geq k $ contains $\binom{q}{k}$ different {\em k-cliques}.
An algorithm that tries to locate the $k$-cliques individually and examine the adjacency
between them can therefore be very very slow for large networks. 
Palla et al make two observations that help one come up with a better strategy.
First, a clique of size $q$ is clearly a $k$-clique connected subset for any $k \leq q$. 
Second, two large cliques that share at least $k-1$ nodes form one $k$-clique connected component as well.
Leveraging these, the strategy of Palla et al. avoids searching for $k$-cliques individually
and instead first locates the large cliques in the network and then looks for the $k$-clique connected subsets of given $k$ (that is, the $k$-clique communities) by studying the overlap between them.
More specifically, their algorithm first constructs a symmetric {\em clique-clique overlap matrix}, 
in which each row represents a large (to be precise a maximal) clique, 
each matrix entry is equal to the number of common nodes between the two corresponding cliques, and each diagonal entry is equal to the size of the clique. 
The $k$-clique communities for a given $k$ are then equivalent to connected 
clique components in which the neighboring cliques are linked to each other by at least $k-1$ common nodes. Palla et al then find these components by erasing every off-diagonal entry smaller than $k-1$ and every diagonal entry smaller than $k$ in the matrix, replacing the remaining elements by one, and then carrying out a component analysis of this matrix. The resulting separate components are equivalent to the different $k$-clique communities.
       

%detects communities by merging overlapping sets of adjacent cliques. The first step in the algorithm is to identify all cliques of size $k$ in the given graph. This is typical done by detecting all maximal cliques in the graph. Then, a new graph is constructed such that each vertex represents one of these $k$-cliques. Two nodes in this new graph are considered to be connected if the $k$-cliques that represent them share $k$?1 members. The cliques of the connected components in the new graph are merged into communities. Since a vertex can occur in more than one $k$-cliques simultaneously, this results in an overlapping communities structure.

%\subsection{Our Method}

{\bf Our Method. } We devised an algorithm based on a similar idea as the procedure
above, but using a variant of our heuristic maximum clique-algorithm 
(Algorithm 2) for the core clique detection step. 

The standard CPM in essence presupposes finding {\em all} maximal cliques in a network.
The number of maximal cliques in a network can in general be exponential in the number of nodes $n$ in the network. In our method, we work instead with a smaller set of maximal cliques. In particular, the basic variant of our method considers exactly one clique per vertex, the largest of all the maximal cliques that it belongs to. Clearly, this can be too restrictive a requirement and may fail to add deserving members to a $k$-clique community. 
To allow for more refined solutions, we include a parameter $c$
in the method that tells us how many additional cliques per vertex would be considered.
The case $c=0$ corresponds to no additional cliques than the largest maximal clique containing the vertex. The case $c\geq1$ corresponds to $c$ additional maximal cliques (each of size at least $k$)
per vertex. Hence, in total, $n(c+1)$ cliques will be collected. For a particular vertex $v$, the largest maximal clique containing it 
can be heuristically obtained by exploring the neighbor of $v$ with {\em maximum degree} 
in the corresponding step in Algorithm 2.
In contrast, to obtain the other maximal cliques involving $v$ (case $c\geq1$),
we explore a {\em randomly} chosen neighbor of $v$, regardless of its degree value.



%The intuition for this follows from the fact that, in  CPM, many $k$-cliques which are $k$-clique-connected are merged into a single community. If we consider one such $k$-clique-community, it would naturally include the largest clique of size greater than or equal to $k$, that can be formed with a subset of its vertices. Hence, all large cliques of size greater than or equal to $k$ would certainly be contained within one of the $k$-clique-communities. 

%Hence, we extended our heuristic such that it returns the largest clique (not just the size) containing each node. However, only considering the largest cliques containing each vertex is clearly not enough, since there could be other vertices in a $k$-clique-community that does not belong to such large cliques, but could still be $k$-clique-connected to it. We need to consider additional maximal cliques in order to include these vertices. Hence, we introduced a parameter $c$, which essentially runs the heuristic $c$ additional times using a random choice of vertex at each step instead of picking the vertex with the largest degree. Assuming no duplicates, in total, $(c+1) \times n$ cliques would be collected, $n$ from the largest-degree-vertex selection iteration, and $c \times n$ from the random-vertex selection iterations. Since it is quite possible to produce duplicate cliques, the total number of cliques collected is typically lesser than $(c+1) \times n$.
%In addition, given a value of $k$, it is not necessary to look for cliques that are smaller than size $k$. We do this by replacing all occurrences of $max$ with $k-1$in Algorithm 2 which discards these. (We also remove Lines 2 and 3 as we now don't need to keep track of the maximum clique size.) Once the cliques are collected, we perform a standard component analysis of the clique-clique overlap matrix to obtain the communities.


%In order to make merging of cliques more ``aggressive'', we also included an option for extracting cliques other than the {\em largest}. 
%win the network since every smaller $k$-clique in it would also be $k$-clique-connected. 

%In order to make merging of cliques more ``aggressive'', we also included an option for extracting cliques other than the {\em largest}. 
%We implemented this by introducing a parameter $c$, which essentially runs the heuristic $c$ additional times using a random choice of vertex at each step instead of picking the vertex with the largest degree. 


%We also modified the pruning steps by discarding cliques that cannot be more than size $k$, instead of the maximum clique size. Since we only look for cliques that overlap at least by $k$ nodes, we can safely prune these out. This is achieved by replacing all occurrences of $max$ in Algorithm 2???, and removing Lines (2 and 3???) as we now don't need to keep track of the maximum clique size. 

%\subsection{Test on Synthetic Networks}

{\bf Test on Synthetic Networks. } We tested our algorithm on the LFR benchmarks\footnote{http://sites.google.com/site/andrealancichinetti/files} proposed in \cite{2008PhRvE..78d6110L}. These benchmarks have the attractive feature that they allow
for generation of synthetic networks with known ``ground truth"  about the communities.
We generated graphs with $n$ = 1000 nodes, average degree $K$ = 10, and power law exponents $\tau_1$ = 2 and $\tau_2$ = 1 in the LFR model. We set the maximum degree $K_{max}$ to 50, and the minimum and maximum community sizes $C_{min}$ and $C_{max}$ to 20 and 50, respectively. We used two far-apart values each for the mixing parameter $\mu$, the fraction of overlapping nodes $O_n$, and the number of communities each overlapping node belongs to $O_m$. Specifically, for $\mu$ we used 0.1 and 0.3, for $O_n$ we used 10$\%$ and 50$\%$, and for $O_m$ we used 2 and 8. 
All together, these combinations of parameters resulted in eight graphs in the testbed.
%, which is of the same order as most large real-world social networks. 
%We chose to vary these as opposed to the others since these would have the most profound impact on the network topology for overlapping community detection. 
%These benchmarks introduce heterogeneity into degree and community size distributions of a network. These distributions are governed by power laws with exponents $\tau_1$ and $\tau_2$, respectively. It allows the generation of overlapping communities using the parameters, $O_n$, the fraction of overlapping nodes and $O_m$, the number of communities each overlapping node is assigned to. It also provides other parameters to control the network topology, such as the mixing parameter $\mu$, which is the expected fraction of links through which a node connects to other nodes in the same community, the average degree $K$, the maximum degree $K_{max}$, the maximum community size $C_{max}$, and the minimum community size $C_{min}$. 

%We evaluated the performance of our algorithm using the {\it Omega Index} \cite{doi:10.1207/s15327906mbr2302_6} which is the overlapping version of the Adjusted Rand Index (ARI) \cite{ARI_paper_1985}.
%which is a metric to compare how well two sets of clusters or communities agree with each other.
%, and it considers pairs of nodes belonging to the same number of clusters (possibly none) to compute the index.

%We define the set of clusters found as a cover $C=\{c_1, c_2, ..., c_k\}$ [Lancichinetti et al. 2009], where $k$ is the number of clusters found.In the case of overlapping communities, a node may belong more than one cluster. Let $N1$ and $N2$ be the number of communities in covers $C1$ and $C2$, respectively, the omega index $\omega$ is defined as [Gregory 2011; Havemann et al. 2011]
%
%\begin{equation}
%\omega(C_1, C_2) = \frac{\omega_u(C_1, C_2) - \omega_e(C_1, C_2)}{1 - \omega_e(C_1, C_2)}
%\label{eqn-omega}
%\end{equation}
%
%The unadjusted omega index $\omega_u$ is defined as
%
%\begin{equation}
%\omega_u(C_1, C_2) = \frac{1}{M} \sum\limits_{i=0}^{max(N_1,N_2)} \left\vert{s_i(C_1)\cap s_i(C_2)}\right\vert 
%\end{equation}
%
%where $M = n(n-1) /2$ represents the number of node pairs and $s_i(C)$ is the set of pairs that appear exactly $i$ times in a cover $C$. The expected omega index in the null model $\omega_e$ is given by
%
%\begin{equation}
%\omega_e(C_1, C_2) = \frac{1}{M^2} \sum\limits_{i=0}^{max(N_1,N_2)} \left\vert{s_i(C_1)}\right\vert \cdot \left\vert{s_i(C_2)}\right\vert 
%\end{equation}
%
%The subtraction of the expected value in Equation \ref{eqn-omega} takes into account agreements resulting from chance alone. The larger the omega index is, the better the matching is between two covers. A value of 1 indicates perfect matching. When there is no overlap, the omega index reduces to the ARI.

We evaluated the performance of our algorithm against the ground truth 
using {\it Omega Index} \cite{doi:10.1207/s15327906mbr2302_6}, 
which is the overlapping version of the Adjusted Rand Index (ARI) \cite{ARI_paper_1985}.
Intuitively, Omega Index measures the extent of agreement between two given sets of communities, by looking at node pairs that occur the same number of times (possibly none) in both.

We used three values for the parameter $c$ of our method: 0, 2, and 5. 
Recall that $c$ set to zero corresponds to picking only the largest clique for each node. 
As $c$ is increased, more and more large cliques (each of size at least $k$)
are considered for each node. 

We compare the performance of our method with that of CFinder\footnote{http://www.cfinder.org}, an implementation of CPM~\cite{cite-key}. We used the command line utility provided in the package for all experiments. For this study, we used a MacBook Pro running OS X v.10.9.2 with 2.66 GHz Intel Core i7 processor with 2 cores, 256KB of L2 cache per core, 4MB of L3 cache and 8GB of main memory.

\input{results_application1}

\input{results_application2}

%The results are shown in Table \ref{tab:applications1} and \ref{tab:applications2}. 

Table~\ref{tab:applications1} shows the experimental results we obtained.
The first three columns of the Table list the various parameters used to generate each test network. The fourth column lists the number of ground truth communities ($C(GT)$) in each network.  The remaining columns in the table show performance 
in terms of the number of communities detected ($C$), 
total number of shared nodes ($S$) and the Omega Index ($\Omega$) for 
CFinder and our method with different values for the parameter $c$.

In our experiments, for CFinder as well as all variants of our algorithm, 
we found that the value of $k=4$ gives the best Omega Index value relative 
to the ground truth. All results reported in Table~\ref{tab:applications1} are therefore
for $k=4$.

In general we observe that there is a close correlation between our method and CFinder
in terms of all three of the quantities $C$, $S$ and $\Omega$. It can be seen that as we increase the value of $c$ in our method, the Omega Index values get closer and closer to that of CFinder. For our algorithm run with $c=0$, the Omega Index is about 75$\%$ of that of CFinder. When run with $c$ = 2, it is about 92$\%$ and for $c$ = 5, it is about 99$\%$. When we increased the value of $c$ even further to 10, we observed that the Omega Index was almost identical to that obtained by CFinder. From this, one can see that we can get almost exactly the same results as the CPM method using our algorithm which uses only a small set of 
maximal cliques, as opposed to all the maximal cliques in the graph.

Table~\ref{tab:applications2} shows the time taken by CFinder and our method 
run with $c$ = 0, 2 and 5. For our method, the table lists the total run time $\tau$ 
as well as the time $\tau_c$ spent on just the clique detection part and the ratio
${\tau_c}/{\tau}$ expressed in percents ($\tau_c$\%).
The remaining time $\tau - \tau_c$ is spent on building the clique-clique overlap matrix, eliminating cliques, component analysis and generating the communities.  As a side note,
we point out that our immediate goal in the implementation of these later phases has been quick experimentation, rather than efficient code, and therefore there is a very large room for improving the runtimes. This is also reflected by the numbers; one can see from the table that the average percentage of time our algorithm spends on clique-finding decreases as $c$ is increased; the quantity is about 30$\%$ when $c$ = 0 and about 16$\%$ when $c$ = 5. Yet, looking only at the total time taken in Table~\ref{tab:applications2}, and comparing CFinder and our algorithm run with $c$ = 5 (the case where the Omega Index values match most closely with CFinder), we see that our algorithm is at least 4$\times$ faster on average. When $c$ is set to 0 and 2, the mean speedups are 51$\times$ and 13$\times$ respectively. 
% Hence, the proportion of time the algorithm spends anything other than clique-detection progressively increases. 

%On the other hand, we noticed that CFinder spent most of its time in finding the cliques. 
Since the source code for CFinder is not publicly available, we were not able to measure exactly the proportion of time it spends on clique-finding. 
We suspect it constitutes a vast proportion of the total runtime.
%However, from our observation during the runs, we estimate this to be, on average, at least more than 95$\%$ of its total runtime. 
%If one assumes this to be 95$\%$, and also assuming the other parts of our algorithm (other than clique-finding) can be optimized to an equal proportion, our algorithm would be $\sim$28$\times$ for the $c$=5 case. 

%\subsection{Test on Real-world Networks}

{\bf Test on Real-world Networks. } We also tested our method and CFinder on four of the real world graphs from our testbed in Section \ref{sec:experiments}---{\em cond-mat-2003}, {\em email-Enron}, {\em dictionary28} and {\em roadNet-CA}---and a user-interest-based graph generated using data collected from Facebook. We briefly explain here how this graph was generated.
Every user on Facebook has a {\it wall}, which is a the user's profile space that allows the posting of messages, often short or temporal notes or comments by other users. We generate a graph with the {\it walls} as vertices, and assign an edge between a pair of vertices, if there is at least one user who has commented on both walls. We assign edge weights proportional to the number of common users, as we consider this to be an indicator of the strength of the connection. A more elaborate explanation of the data collection method and network generation  for the Facebook graph is discussed in \cite{dianapaper}. As this is a weighted graph, we used a threshold (of 0.009) to include only strong links, resulting in a network with 1144 vertices and 2561 edges.
%{\it walls}, and use this to formulate the network for our experiments.

The results of our method for $c$ = 0 and 5, $k$ = 4, and CFinder for the case $k$ = 4 on all the five real-world graphs is shown in  Table~\ref{tab:applications3}. The table lists the number of communities found by each algorithm, total number of shared nodes, and the total time taken. For our algorithm, it also specifies the time spent on clique-finding. These graphs do not have any known community structure, so we were not able to measure the Omega Index values. For the graph {\em email-Enron}, CFinder was not able complete within 30 minutes. For the $c$ = 0 case, our algorithm comfortably outperforms CFinder, even in terms of total time, on average performing about 115$\times$ faster. For the $c$ = 5 case, the total time taken by our algorithm turns out to be higher for some cases, and lower for others. However, it should be noted that the time taken for clique finding is incomparably smaller than the post-clique-finding processing time. 
%If we make a previous assumption of 5$\%$ of the total time to which the post-clique-processing time can be reduced to, the speedup we gain would be on average, $\sim$576$\times$ faster for the $c$=0 case and $\sim$66$\times$ faster for the $c$=5 case.

\input{results_application3}









%% (ref to supplementary info???).  
%
%
%We developed a very simple clique-based community detection algorithm using our heuristic as follows. We modified our heuristic such that it retains the largest clique containing each node. We use the heuristic instead of the exact algorithm because it is much faster and delivers near-optimal solution which is sufficient for the purpose of detecting communities. It is possible that we get duplicate cliques via different nodes,  
%and so we remove these. The resulting cliques are cohesively connected subgroups, and those are
%what our algorithm outputs as communities.
%
%%This is done by simply substituting Lines 3 and 4 of Algorithm \ref{alg:clqHeu}, with a simple routine that stores each clique in a preferred data structure. 
%
%
%%A detailed overview of community detection methods, and the significance and complexity involved in overlapping community findingcan be found in \cite{Fortunato_2010}.
%
%
%%\footnotetext{http://www.facebook.com}
%
%For experiments, we generated a user-interest-based network using data collected from 
%two widely used social media platforms: Facebook and Twitter.
%%\footnote[1]{http://www.facebook.com}  and Twitter \footnote[2]{http://www.twitter.com}. 
%Facebook {\it walls} and Twitter {\it profiles} are features in these platforms that provide a medium for businesses, groups and individuals to post content such as messages, promotions or campaigns. These features also engage other users by allowing them to reply or comment on the already posted content. We use users' comments on posts added on the Facebook {\it walls} as an indicator of their interests in the respective 
%{\it walls}, and use this to formulate the network for our experiments. The user comments and user information from specific {\it walls} are publicly available and collected using Facebook API\footnote[3]{http://developers.facebook.com/}. Similarly, for Twitter, we deduce users' interests by using their {\it tweets}. 
%A {\it tweet} is a message with up to 140 characters related to a particular Twitter {\it profile}. We use two kinds of {\it tweets}: {\it retweet}, which is a {\it tweet} made by a Twitter {\it profile} that gets tweeted again by another interested user, and {\it mentioned tweet}, which is a {\it tweet} made by an interested user regarding the Twitter profile. The publicly available user mentioned tweets, retweets of a Twitter profile, and information of users who tweeted on these profiles are collected using Twitter API\footnote[4]{https://dev.twitter.com/docs/}.
%
%
%
%With the collected data, we use the following technique to generate our user-interest-based network. From the gathered Facebook data, we calculate, for each {\it wall} $i$, the total number of unique users $u_i$ who have commented on it, and for each pair of {\it walls} $i, j$, the number of common users who have commented on both {\it walls} $c_{ij}$. The same procedure is carried out for Twitter {\it profiles}. We then construct an undirected graph $G = (V,E)$, where $V$ represents the {\it walls} or {\it profiles} and $E$ represents edges between them. We assign an edge between a pair of vertices, if there is at least one common user between them. The edge weight $w_{ij}$, which indicates the strength of the connection, is computed using the 
%Jacard index or similarity coefficient \cite{Leydesdorff} given by
%%\begin{equation}
%\[w_{ij} = \frac{c_{ij}}{u_i+u_j-c_{ij}}\]
%%\end{equation}
%
%It is clear from the above equation that the weight is between 0 and 1, a value closer to 1 implying the {\it walls}/{\it profiles} being more similar in terms of user interests.
%We then convert the graph to an unweighted graph by pruning edges whose weights are below a specified threshold, thus retaining only the links that indicate strong correlation. The choice of the threshold is subjective, and determines the size, number and quality of communities detected by our algorithm. 
%
%The weighted graph we formed using the Facebook data had 1144 vertices and 348,274, and that the Twitter graph had 1144 vertices and 204,131 edges. Since we look for the most cohesive subgroups, we set the threshold to a high number, that gave us fairly small communities with up to $\sim$15 vertices.
%
%
%%For our small experiment, we use data collected from Facebook\footnote[1]{http://www.facebook.com}. Every user on Facebook has a {\it wall}, which is a the user's profile space that allows the posting of messages, often short or temporal notes by other users. The user comments and user information from specific {\it walls} are publicly available and we collected them using Facebook API. We constructed a graph with the {\it walls} as vertices. Any two users who have commented on the same {\it wall} indicate a connection between the {\it walls}, and we form an edge between them. There could be many common users for each wall, and so we assigned edge weights by Jacard index or similarity coefficient \cite{Leydesdorff}. Once this is done for all {\it walls}, we retained only those edges which have weights above a chosen threshold, indicating a strong correlation. The threshold is a user's choice and decides both the size and the number of communities found.
%% If an edge already exists, we increase the edge weight by 1. Once this is done for all {\it walls}, the edge weights are normalized, and we retain only those edges which have edge weights above a chosen threshold (we use 0.01), indicating a strong correlation.
%
%%duplicate removal
%%We modified our heuristic to retain the largest maximum clique containing each node. 
%%This is done by simply substituting Lines 3 and 4 of Algorithm \ref{alg:clqHeu}, with a simple routine that stores each clique in a preferred data structure. 
%%The exact algorithm could have also been used instead of the heuristic for this purpose. We choose the heuristic since it is much faster and for this particular problem of community detection the accuracy of the size of cliques formed is not critical.


\begin{figure}%[h!]
  \centering
    %\includegraphics[width=1\textwidth]{communities_fb.pdf}
    \includegraphics[scale=0.5]{communities_fb_new.pdf}
    
%\vspace{-30pt}
  \caption{Some Facebook communities detected by our algorithm.}
\label{fig-communities-fb}
\end{figure}
%\vspace{-10pt}

We also present some of the sports-related communities detected by our algorithm in the Facebook graph in Figure \ref{fig-communities-fb}, which is self-explanatory.
%We obtained a number of isolated communities similar to the one of {\it popular musicians}, and that of {\it internet corporations}. We also see communities that pertain to {\it MSNBC and its (news related) shows}, {\it news channels}, and {\it politics}. One can see that although the {\it news channels} and {\it MSNBC shows} communities are essentially separate, they share a common member, {\it MSNBC}, which is relevant to both. Similarly the {\it news channels} and {\it politics} communities share two nodes. One can see from that overlapping communities are a natural characteristic of real-world networks, which our algorithm helps in detecting.


%A few cautionary points need to be pointed out here. The figure represents only a small subset of the communities detected by our algorithm, which we decided to present here to illustrate the results of our algorithm. In reality, a large number of communities similar to the ones shown here were detected. Furthermore, the members in the communities also represent only a subset of nodes that belonged to that community, and we decided not to show the rest to favor clarity and simplicity. In order to improve readability, we have also decrypted the names of the nodes, which in reality were Facebook or Twitter handles of {\it walls} or {\it profiles}. The original handles lacked spaces in between words, had special characters such as underscore, or had additional terms, all of which we decided to leave out to help the reader better comprehend the names.

%As shown in Figure \ref{fig-communities-fb}, we obtained a number of isolated communities similar to the one of {\it popular singers}, and that of {\it internet corporations}. We also see communities that pertain to {\it MSNBC and its (news related) shows}, {\it news channels}, and {\it politics}. The point of this experiment is that our algorithm allows a node to be a member of more than one community and therefore helps detect overlapping communities. For instance, although the {\it news channels} and {\it MSNBC shows} communities are essentially separate, they share a common member, {\it MSNBC}, which is relevant to both. 
%
%A similar feature can be observed between the {\it news channels} and  {\it politics} communities
%where the two communities share two members. 
%Figure \ref{fig-communities-tw} shows the communities detected from the Twitter graph. 
%One can observe a very similar pattern there as well.
%%add from nature paper why this is significant. coexistence of their structural subunits 
%
%%\vspace{-20pt}
%\begin{figure}%[h!]
%  \centering
%    %\includegraphics[width=1\textwidth]{communities_tw.pdf}
%    \includegraphics[scale=0.25]{communities_tw.pdf}
%%\vspace{-30pt}
%  \caption{Some Twitter communities detected by our algorithm.}
%\label{fig-communities-tw}
%\end{figure}
%%\vspace{-10pt}
%

%As demonstrated by these experiments, the algorithm used here although rather elementary can be effective at detecting overlapping communities. Our algorithm, however, detects communities formed by cliques, which is a stringent requirement. Appropriate relaxations, such as the use of $k$-cores, is worth investigating and is one of the future lines of work we intend to pursue. 




%The immediate next thing to try would be to extend this to allow for clique-relaxations, such as $k$-core, $k$-plex \ref{Doreian1994267} etc. This can be done by merging the communities detected by our algorithm, which is one of the future lines of work we intend to pursue.
